# =============================================================================
# Extraction Job Environment Variables
# =============================================================================
# Copy this file to .env and fill in your values
# cp .env.example .env

# =============================================================================
# GCP Project Settings (Required)
# =============================================================================
GCP_PROJECT=your-gcp-project-id
GCP_REGION=us-central1

# =============================================================================
# Extraction Model Settings
# =============================================================================
# Primary model for PDF extraction (Gemini 2.5 models recommended)
EXTRACTION_MODEL=gemini-2.5-flash-lite

# Fallback models (used if primary fails)
EXTRACTION_FALLBACK_MODEL=gemini-2.5-flash
EXTRACTION_FALLBACK_MODEL_2=gemini-2.5-pro

# Number of PDF pages per chunk for LLM processing
EXTRACTION_CHUNK_SIZE=10

# HTTP request timeout in seconds
ASYNC_HTTP_REQUEST_TIMEOUT=60

# =============================================================================
# Embedding Settings
# =============================================================================
# Gemini embedding model (gemini-embedding-001 recommended)
# - Up to 3072 output dimensions (configurable)
# - 2048 token max sequence length
# - Excellent multilingual support (Indonesian + English)
EMBEDDING_MODEL=gemini-embedding-001

# Embedding vector dimension (768, 1024, or up to 3072 for gemini-embedding-001)
# Using full 3072 dimensions for maximum semantic precision
EMBEDDING_DIMENSION=3072

# Maximum texts per batch request
EMBEDDING_BATCH_SIZE=250

# Maximum text length before truncation (characters)
# ~4 chars per token, so 8000 chars â‰ˆ 2000 tokens (within 2048 limit)
EMBEDDING_MAX_TEXT_LENGTH=8000

# Character overlap between chunks for long documents
EMBEDDING_CHUNK_OVERLAP=500

# Enable/disable embedding generation (true/false)
ENABLE_EMBEDDINGS=true

# Enable chunked embeddings for long documents (true/false)
ENABLE_CHUNK_EMBEDDINGS=false

# =============================================================================
# Database Settings (Cloud SQL with pgvector)
# =============================================================================
# PostgreSQL connection string with pgvector extension
# Format: postgresql://user:password@host:5432/database
# For Cloud SQL: postgresql://user:password@/database?host=/cloudsql/PROJECT:REGION:INSTANCE
DATABASE_URL=postgresql://postgres:gr*?x)JmN&n([_`&@localhost:5432/legal_council

# Connection pool settings
DATABASE_POOL_MIN_SIZE=1
DATABASE_POOL_MAX_SIZE=10
DATABASE_COMMAND_TIMEOUT=60

# Enable/disable database storage (true/false)
# If false, extraction results will only be returned, not stored
ENABLE_DATABASE_STORAGE=true

# =============================================================================
# LiteLLM Settings (for extraction models)
# =============================================================================
# If using Vertex AI models via LiteLLM, these may be needed:
# VERTEXAI_PROJECT=your-gcp-project-id
# VERTEXAI_LOCATION=us-central1

# For API-based models (if not using Vertex AI):
# GEMINI_API_KEY=your-api-key
# OPENAI_API_KEY=your-api-key
